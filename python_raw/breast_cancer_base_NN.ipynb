{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8da28b-5eae-48c0-97d0-456980bc1e12",
   "metadata": {},
   "source": [
    "# Métodos de Compresión - Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832cbef-ee0c-4e08-806f-94d954f7004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#\tMODEL COMPRESSION TECHNIQUES EXAMPLE. 2022.\n",
    "######################################################################\n",
    "# Copyright (C) 2022. J.D.Diaz-Delgado (JDD) jd.diazd@uniandes.edu.co\n",
    "# \n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, version 3 of the License.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>\n",
    "####################################################################*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941442ca-227a-49e6-933a-95f3cefdddac",
   "metadata": {},
   "source": [
    "## Introducción a las Técnicas de Compresión\n",
    "\n",
    "Las técnicas de compresión son utilizadas para reducir el tamaño de modelos de machine learning con el objetivo de mejorar eficiencia, evitar la sobre-parametrización y optimizar el uso de recursos. Existen diferentes técnicas de compresión, en este caso se usa la poda de parámetros, Knowledge Distillation y la Cuantificación. A continuación se construirá el modelo base y posteriormente se aplicarán estas técnicas para reducir su tamaño.\n",
    "\n",
    "El modelo que se utiliza en este ejemplo se encarga de realizar una clasificación binaria entre benigno y maligno de la biopsia de una masa en el pecho de un paciente. Se usan 30 características con el fin de determinar si es cancerigeno o no. Este dataset de 569 instancias, con el que se entrenarán y validarán los modelos, le pertenece a la Universidad de Wisconsin y se puede encontrar [aquí](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ce21d-7909-4ce7-91c4-11d0f08988e5",
   "metadata": {},
   "source": [
    "## Construcción del Modelo Base\n",
    "En esta sección nos dedicaremos a hacer el preprocesamiento de los datos y a generar el primer modelo del cual partiremos para usar las técnicas de compresión. Empezaremos importando las librerias que usaremos en este ejemplo. La construcción de este modelo base hace parte de un tutorial realizado por Dr. Sreenivas Bhattiprolu que se puede encontrar [aquí](https://colab.research.google.com/drive/1WEZxybgoxQz8Lmp_r6Zq6OHYdvwaz2Df?usp=sharing)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b65515-0b01-4e77-8ce9-5c450ebd59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f30611-e99d-4d13-af66-ffa7396e0f1d",
   "metadata": {},
   "source": [
    "Se tiene un archivo:\n",
    "\n",
    "`data.csv`: posee las 30 características de los núcleos de las células tomadas de una biopsia por aspiración con aguja fina sobre cada usuario, identificado con un ID único. Son 10 características principales de las cuales se toma la media (*_mean*), el error estándar (*_se*) y el peor resultado encontrado en la biopsia (*_worst*). Puede observar los nombres de las 10 características principales a continuación:\n",
    "\n",
    "- ID: número de paciente\n",
    "- DIAGNOSIS: resultado del diagnóstico; Benigno (B) o Maligno (M)\n",
    "\n",
    "- RADIUS: distancia desde el centro a puntos en el perímetro\n",
    "- TEXTURE: textura\n",
    "- PERIMETER: perimetro\n",
    "- AREA: área\n",
    "- SMOOTHNESS: variación local en tamaños del radio\n",
    "- COMPACTNESS: perimetro^2 / área - 1.0\n",
    "- CONCAVITY: severidad de las porciones cóncavas del contorno\n",
    "- CONCAVE_POINTS: número de porciones cóncavas del contorno\n",
    "- SYMMETRY: simetría\n",
    "- FRACTAL_DIMENSION: dimensión fractal (aproximación de línea de costa - 1)\n",
    "\n",
    "A continuación se muestran las primeras instancias del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508c61a-2c5a-4d13-9891-4412d56be8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c4cba-5203-4b27-9df9-da58fbc162ed",
   "metadata": {},
   "source": [
    "Vemos que la columna 32 no tiene ninguna característica asignada y ninguno de los datos de esta columna aporta valor. Por esta razón podemos retirarla y volvemos a imprimir las primeras filas para verificar el cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c470299-5250-4817-b6e0-317fbc6437e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 32', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fbf44-4ce0-4d1e-97e5-5e1e5b7ca9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446bb81-b5a0-498e-acb7-ebbd617475ab",
   "metadata": {},
   "source": [
    "Una buena práctica es verificar si hay valores faltantes por cada característica para tomar la decisión de retirar una característica por una gran falta de valores o retirar filas específicas que no cuenten con todos los valores. Podemos verificar esto usando `df.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82165a-902f-45bb-b87d-34ba47ad0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc92d4e-119c-4c50-8a48-400ed4ad2da1",
   "metadata": {},
   "source": [
    "En este caso no es necesario eliminar ninguna columna debido a que todas contienen datos. Pasamos ahora a renombrar la columna de `diagnosis` a `label` para tener claro que esta columna contiene el resultado esperado de la clasificación. Adicionalmente, graficaremos la distribución de ambas clases para determinar si hay un desbalance significativo de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b63430-bc9f-489a-b979-50604e959b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'diagnosis' : 'label'})\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417263ab-0fd1-45a4-bfc4-1f363db5973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plots = sns.countplot(x = \"label\", data = df)\n",
    "print(\"Distribución de los datos: \", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c1681-f78d-4e5f-93cb-b98e90eaff96",
   "metadata": {},
   "source": [
    "En la gráfica podemos observar que existe un desbalance entre clases cercano a la razón 4:6. Sin embargo, es un desbalance leve por lo que no es necesario darle un tratamiento diferente. Continuamos con la codificación de etiquetas para hacer el cambio de 'B' y 'M' a 0 y 1, respectivamente. Esto para poder dar un valor a las clases y entrenar el modelo. Para la codificación usamos `LabelEncoder()`. Luego de completada la codificación y de definir `y` como la columna `label`, podemos retirar esta columna y la de `id` (que no aporta nada al entrenamiento) para preparar los datos para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c56fe-7690-4b12-a17e-a0eacfdb73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "print(\"Las etiquetas antes de la codificación son: \", np.unique(y))\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(y)\n",
    "print(\"Las etiquetas luego de la codificación son: \", np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d083ec-f4b4-4d47-8b59-aabcc6466b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels = ['label', 'id'], axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf110b02-cf96-4ffe-b047-d17497f169a7",
   "metadata": {},
   "source": [
    "En la tabla que acabamos de imprimir vemos que las características toman valores en rangos muy variados: radio entre 17 y 20, perimetro entre 77 y 135, area entre 386 y 1326, etc. Esto hace que sea más dificil el proceso de entrenamiento, por esta razón usaremos la función `MinMaxScaler()` que se encarga de escalar cada característica de forma individual a un rango entre 0 y 1 con el objetivo de obtener un modelo con mejor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96197a8-2a12-4ec3-9f42-c47baf6f8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9d637-4431-4ad2-b8ad-17f714a24887",
   "metadata": {},
   "source": [
    "Ahora haremos el split de datos de entrenamiento y de validación. Tomaremos el 25% del dataset para validación y el resto para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c787a64-998a-41e9-826e-ed03aa11fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n",
    "print(\"La forma de los datos de entrenamiento es: \", X_train.shape)\n",
    "print(\"La forma de los datos de validación es: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd1962-3a46-4cbd-8d00-65293e06456b",
   "metadata": {},
   "source": [
    "Ya con la partición de los datos en entrenamiento y validación, podemos crear nuestro modelo de red neuronal. Para este ejemplo usaremos Keras y la armaremos de forma secuencial, lo que significa que añadiremos capa por capa. Tendremos una red neuronal totalmente conectada con 30 neuronas de entrada (para las 30 características), luego una capa oculta de 16 neuronas y por último una neurona de salida. Adicionalmente usamos una capa de `Dropout(0.2)`, entre la capa oculta y la de salida, para que de forma aleatoria el 20% de los pesos se vayan a cero para prevenir un sobreajuste a los datos. Como función de activación se usará una sigmoide. \n",
    "\n",
    "Imprimimos el resumen del modelo en el que se muestra cada una de las capas y la cantidad de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d834e-443a-4ed3-b388-bc7a3a619d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim = 30, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494c6a0-f0d8-46f0-ad61-4e869da0880f",
   "metadata": {},
   "source": [
    "Luego de haber terminado con el preprocesamiento y de haber escogido una arquitectura para nuestra red neuronal podemos empezar el entrenamiento!! Este proceso lo guardaremos en la variable `history_base` para luego poder analizarlo. Entrenaremos la red por 100 epochs usando un batch size de 64. Si hay problemas de memoria, pueden disminuír el batch size en potenicas de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251ff41-8bb7-459e-ae99-ec61e1c614c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_base = model.fit(X_train, y_train, verbose = 1, epochs = 100, batch_size = 64, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7425a-d430-4cdf-a9c7-67f4f7de2033",
   "metadata": {},
   "source": [
    "Finalizado el entrenamiento, podemos analizar el comportamiento de las pérdidas y la precisión a medida que avanzamos de epoch. A continuación podremos ver 2 gráficas relacionadas a estos indicadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c9d50-e8ed-4067-82b7-4e4a87182a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history_base.history['loss']\n",
    "val_loss = history_base.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Pérdida de entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'b', label='Pérdida de validación')\n",
    "plt.title('Pérdida de entrenamiento y validación')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.savefig('perdida_modelo_base.png')\n",
    "plt.show()\n",
    "\n",
    "acc = history_base.history['accuracy']  # Use accuracy si acc no funciona\n",
    "val_acc = history_base.history['val_accuracy']  # Use val_accuracy si acc no funciona\n",
    "plt.plot(epochs, acc, 'g', label='Precisión de entrenamiento')\n",
    "plt.plot(epochs, val_acc, 'b', label='Precisión de validación')\n",
    "plt.title('Precisión de entrenamiento y validación')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.savefig('precision_modelo_base.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71637336-9dbc-4c53-a95d-d9498c53e294",
   "metadata": {},
   "source": [
    "Adicionalmente, podemos graficar la matríz de confusión que nos muestra que tan bueno es el modelo prediciendo para ambas clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b2362-6519-4a9d-8763-8d9e491f5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir los resultados del bloque de validación\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Hacer la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "matriz_modelo_base = sns.heatmap(cm, annot=True, cmap=\"crest\")\n",
    "fig = matriz_modelo_base.get_figure()\n",
    "fig.savefig(\"matriz_modelo_base.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ef554-0ad6-4cf2-ad60-7b684910337b",
   "metadata": {},
   "source": [
    "## Técnica de Compresión - Poda de parámetros\n",
    "Ya teniendo nuestro modelo base construido, podemos pasar a aplicar una de las técnicas de compresión para disminuir el tamaño del modelo. Empezaremos con la poda de parámetros. Esta técnica consiste en eliminar de forma iterativa los parámetros con menor peso, es decir, eliminar los parámetros que cuentan con el menor poder predictivo en el modelo. De esta manera, podremos llevar muchos de los pesos a 0 sin que el modelo pierda mucha precisión. En este ejemplo usaremos la librería de `tensorflow_model_optimization`. El uso de esta técnica está basado en el ejemplo de TensorFlow que se puede encontrar [aquí](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)<br>\n",
    "\n",
    "Primero haremos un reconteo de los parámetros capa por capa y los imprimiremos para ver sus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff245ab7-1fd1-4e9b-b2f2-ac9718d56fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for element in model.layers[0].get_weights()[0]:\n",
    "    for i in element:\n",
    "        # if i != 0: count += 1\n",
    "        count += 1\n",
    "for j in model.layers[0].get_weights()[1]:\n",
    "    # if j != 0: count +=1\n",
    "    count += 1\n",
    "    \n",
    "for element in model.layers[2].get_weights()[0]:\n",
    "    for i in element:\n",
    "        count += 1\n",
    "        \n",
    "for j in model.layers[2].get_weights()[1]:\n",
    "    count += 1\n",
    "\n",
    "print(\"Total número de parámetros: \", count)\n",
    "print()\n",
    "\n",
    "weights = model.get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989e0b3-d901-40d4-861e-79cdbabb92b6",
   "metadata": {},
   "source": [
    "Observamos que tenemos 513 parámetros en total y la gran mayoría de estos pesos es diferente de cero. Ahora guardaremos el modelo base en formato `SavedModel` que es el formato que recomienda TensorFlow en vez de la versión comprimida `.h5`. Para la implementación en FPGA se guardarán los pesos y biases en archivos `.csv`. Es importante eliminar la primera fila y la primera columna de estos archivos para no tener problemas en la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334a83c-cd96-47a2-9059-2dde12c65a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Precisión modelo base:', baseline_model_accuracy)\n",
    "\n",
    "# Modelo Base - Formato SavedModel\n",
    "model.save(\"modelo_base\")\n",
    "\n",
    "# Archivos de Pesos y Biases para implementación en FPGA\n",
    "w1_base = model.layers[0].get_weights()[0]\n",
    "b1_base = model.layers[0].get_weights()[1]\n",
    "w2_base = model.layers[2].get_weights()[0]\n",
    "b2_base = model.layers[2].get_weights()[1]\n",
    "\n",
    "pd.DataFrame(w1_base).to_csv(\"w1_base.csv\")\n",
    "pd.DataFrame(b1_base).to_csv(\"b1_base.csv\")\n",
    "pd.DataFrame(w2_base).to_csv(\"w2_base.csv\")\n",
    "pd.DataFrame(b2_base).to_csv(\"b2_base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bfe8b-9292-4580-a67c-b92245b5ab35",
   "metadata": {},
   "source": [
    "Adicionalmente, haremos la conversión de este modelo a TensorFlowLite para hacer la implementación en el microcontrolador. No se usará optimización en el proceso de conversión para poder compararlo posteriormente con los modelos cuantificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9670a-d977-4ac5-b48c-a7546c1d8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión del modelo\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"modelo_base\")\n",
    "tflite_model = converter.convert()\n",
    "open(\"modelo_base.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6139b-e2db-48ec-9fb8-26680228ee36",
   "metadata": {},
   "source": [
    "Ahora usaremos parte del ejemplo dado por TensorFlow para crear el modelo que vamos a podar con sus respectivos parámetros de poda. En este caso vamos a entrenar el modelo por la misma cantidad de epochs (100), mismo batch size (64) y mismo porcentaje para bloque de validación (25%) que el modelo base. Adicionalmente, iniciaremos con un sparsity del 30% y finalizaremos con un sparsity del 60%. Esto quiere decir que se removerá luego de la primera iteración el 30% de los parámetros y se seguirán removiendo iterativamente hasta llegar al 60% en el último epoch. Como salida de este bloque de código tenemos un resumen del modelo que usaremos para podar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac11ce4-a8a2-4773-b039-cf7305ad80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Calcule el paso final para finalizar la poda luego de 100 epochs.\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "validation_split = 0.25 # 25% de los datos se usará para validación. \n",
    "\n",
    "X_num = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(X_num / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Defina el modelo para podar.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.30,\n",
    "                                                               final_sparsity=0.60,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requiere una recompilación.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66558a1f-a107-4912-95a1-6b6e4752547b",
   "metadata": {},
   "source": [
    "Ahora pasamos a entrenar el modelo para podar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4532e-93bc-4015-ab6e-b85992602777",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "]\n",
    "\n",
    "history_poda = model_for_pruning.fit(X_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6fa65-2119-49d9-9417-292286fa8a8d",
   "metadata": {},
   "source": [
    "Luego de entrenar este modelo, podemos pasar a analizarlo con las mismas gráficas que mostramos para el modelo base y mostrando la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0491b5-97d9-45de-a7d5-700c73853ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history_poda.history['loss']\n",
    "val_loss = history_poda.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Pérdida de entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'b', label='Pérdida de validación')\n",
    "plt.title('Pérdida de entrenamiento y validación')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.savefig('perdida_modelo_podado.png')\n",
    "plt.show()\n",
    "\n",
    "acc = history_poda.history['accuracy']  # Use accuracy si acc no funciona\n",
    "val_acc = history_poda.history['val_accuracy']  # Use val_accuracy si acc no funciona\n",
    "plt.plot(epochs, acc, 'g', label='Precisión de entrenamiento')\n",
    "plt.plot(epochs, val_acc, 'b', label='Precisión de validación')\n",
    "plt.title('Precisión de entrenamiento y validación')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.savefig('precision_modelo_podado.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fae538-0c43-4487-8c13-427fc7e6eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir los resultados del bloque de validación\n",
    "y_pred_poda = model_for_pruning.predict(X_test)\n",
    "y_pred_poda = (y_pred_poda > 0.5)\n",
    "\n",
    "# Hacer la matriz de confusión\n",
    "cm_poda = confusion_matrix(y_test, y_pred_poda)\n",
    "\n",
    "matriz_poda = sns.heatmap(cm_poda, annot=True, cmap=\"crest\")\n",
    "fig = matriz_poda.get_figure()\n",
    "fig.savefig(\"matriz_modelo_podado.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e6166-61aa-4aaf-8d52-e3be4cb38f82",
   "metadata": {},
   "source": [
    "Observamos que durante el proceso de entrenamiento, cerca al epoch 20, 40, 60 y 80 hay una pérdida en precisión y un aumento en pérdida significativo. Esto se da porque en estos epochs se removió un parámetro con un poder predictivo considerable. Sin embargo, el modelo aprende a predecir con los parámetros que le quedan y recupera su precisión rápidamente. En la matriz de confusión vemos que el comportamiento es muy similar al modelo base, por lo que no hay una pérdida en desempeño significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8c4ac-16c6-4f03-b035-696af991f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   X_test, y_test, verbose=0)\n",
    "\n",
    "print('Precisión del modelo base:', baseline_model_accuracy)\n",
    "print('Precisión del modelo podado:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae65bb9-5698-4520-b683-eac519a935cf",
   "metadata": {},
   "source": [
    "Vemos que al evaluar el modelo con el set de validación, no hay impacto en la precisión respecto al modelo base. Esto nos muestra que la poda de parámetros ha sido muy efectiva y vale la pena hacer uso de este modelo comprimido. A continuación haremos un conteo de los parámetros de la red diferentes a cero para ver cuál fue el impacto de la compresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e566d-59bd-4b3f-ab97-a2d85b025f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for element in model_for_pruning.layers[0].get_weights()[0]:\n",
    "    for i in element:\n",
    "        if i != 0: count += 1\n",
    "for j in model_for_pruning.layers[0].get_weights()[1]:\n",
    "    if j != 0: count +=1\n",
    "    \n",
    "for element in model_for_pruning.layers[2].get_weights()[0]:\n",
    "    for i in element:\n",
    "        if i != 0: count += 1\n",
    "        \n",
    "for j in model_for_pruning.layers[2].get_weights()[1]:\n",
    "    if j != 0: count += 1\n",
    "\n",
    "print(\"Total número de parámetros: \", count)\n",
    "print()\n",
    "\n",
    "weights = model_for_pruning.get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f0e12-3686-42ad-9ba6-c3e01364c1da",
   "metadata": {},
   "source": [
    "Vemos que el número de parámetros diferentes a cero se redujo drásticamente. Esto se puede evidenciar en el conteo de parámetros y visualmente al imprimir los pesos del modelo podado. Ahora guardaremos el modelo base en formato `SavedModel` que es el formato que recomienda TensorFlow en vez de la versión comprimida `.h5`. Para la implementación en FPGA se guardarán los pesos y biases en archivos `.csv`. Es importante eliminar la primera fila y la primera columna de estos archivos para no tener problemas en la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3775f-677c-446d-8cf0-a223daf14d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Podado - Formato SavedModel\n",
    "model_for_pruning.save(\"modelo_podado\")\n",
    "\n",
    "# Archivos de Pesos y Biases para implementación en FPGA\n",
    "w1_podado = model_for_pruning.layers[0].get_weights()[0]\n",
    "b1_podado = model_for_pruning.layers[0].get_weights()[1]\n",
    "w2_podado = model_for_pruning.layers[2].get_weights()[0]\n",
    "b2_podado = model_for_pruning.layers[2].get_weights()[1]\n",
    "\n",
    "pd.DataFrame(w1_podado).to_csv(\"w1_podado.csv\")\n",
    "pd.DataFrame(b1_podado).to_csv(\"b1_podado.csv\")\n",
    "pd.DataFrame(w2_podado).to_csv(\"w2_podado.csv\")\n",
    "pd.DataFrame(b2_podado).to_csv(\"b2_podado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087254c7-f452-4fe4-bb2c-080d43a9fc57",
   "metadata": {},
   "source": [
    "Adicionalmente, haremos la conversión de este modelo a TensorFlowLite para hacer la implementación en el microcontrolador. No se usará optimización en el proceso de conversión para poder compararlo posteriormente con los modelos cuantificados. Debido a que el modelo podado no quedó bien guardado porque contaba con parámetros basura que se incluyeron durante el proceso de poda, se genera a continuación otro modelo con la misma arquitectura y se le cargan los parámetros relevantes para poder hacer la conversión del modelo correctamente. El modelo arreglado quedará guardado en la carpeta `modelo_podado_fix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0347a-9d15-4fcf-a805-38df1d4a48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_podado_fix = keras.Sequential()\n",
    "modelo_podado_fix.add(keras.layers.Dense(16, input_dim = 30, activation = 'relu', trainable=False))\n",
    "modelo_podado_fix.add(keras.layers.Dropout(0.2))\n",
    "modelo_podado_fix.add(keras.layers.Dense(1, trainable=False))\n",
    "modelo_podado_fix.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "modelo_podado_fix.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "modelo_podado_fix.layers[0].set_weights(model_for_pruning.layers[0].get_weights())\n",
    "modelo_podado_fix.layers[2].set_weights(model_for_pruning.layers[2].get_weights())\n",
    "\n",
    "# Modelo Podado Arreglado - Formato SavedModel\n",
    "modelo_podado_fix.save(\"modelo_podado_fix\")\n",
    "\n",
    "# Conversión del modelo\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"modelo_podado_fix\")\n",
    "tflite_model = converter.convert()\n",
    "open(\"modelo_podado.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# print(modelo_podado_fix.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463e08e-b026-4603-a580-db785da55016",
   "metadata": {},
   "source": [
    "## Técnicas de Compresión - Knowledge Distillation\n",
    "\n",
    "Esta técnica de compresión fue propuesta por Google en el 2015 y consiste en entrenar una red neuronal profesora para luego transferirle el conocimiento a una red neuronal estudiante que tiene un menor tamaño. Este método se basa en la idea de que es posible comprimir el conocimiento y reducir drásticamente el tamaño de los modelos sin comprometer la capacidad de predicción. Los resultados obtenidos por Google fueron usando el dataset de MNIST. En este caso, trataremos de comprimir el conocimiento de nuestro modelo base y mantener una buena precisión. Iniciaremos definiendo la clase `Distiller()` que modifica las funciones `train_step`, `test_step` y `compile()`. El tutorial de esta técnica está basado en la implementación de Kenneth Borup. Por lo tanto, el uso de esta clase y de la técnica en general está detallado allí. \n",
    "\n",
    "La implementación de Kenneth Borup se puede encontrar [aquí](https://keras.io/examples/vision/knowledge_distillation/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8016a5b-2591-4729-ae15-71724a942cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa29a2-ac44-43c9-a0c3-49fed75d7ad7",
   "metadata": {},
   "source": [
    "Luego de haber definido la clase `Distiller()`, pasamos a crear los modelos estudiante y profesor. Estos modelos son creados usando `Sequential()`. Inicialmente vamos a definir la red profesora con 16 neuronas en la capa oculta (como en el modelo base) y la red estudiante con 5 neuronas en la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca0f94-7d94-4000-96a5-ff7497f322d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "capa_oculta_profesora = 16 # Cantidad de neuronas en la capa oculta de la red profesora\n",
    "capa_oculta_estudiante = 5 # Cantidad de neuronas en la capa oculta de la red estudiante\n",
    "\n",
    "# Crear red profesora\n",
    "teacher = keras.Sequential(name = \"teacher\")\n",
    "teacher.add(keras.layers.Dense(capa_oculta_profesora, input_dim = 30, activation = 'relu'))\n",
    "teacher.add(keras.layers.Dropout(0.2))\n",
    "teacher.add(keras.layers.Dense(1))\n",
    "teacher.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Crear red estudiante\n",
    "student = keras.Sequential(name = \"student\")\n",
    "student.add(keras.layers.Dense(capa_oculta_estudiante, input_dim = 30, activation = 'relu'))\n",
    "student.add(keras.layers.Dropout(0.2))\n",
    "student.add(keras.layers.Dense(1))\n",
    "student.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Clonar red estudiante para comparación posterior\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b69a1a-2602-4aca-b9a4-a18828bfe313",
   "metadata": {},
   "source": [
    "Ya definidos los dos modelos, podemos pasar al entrenamiento de la red profesora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf50630-c232-46aa-b720-fd1b0b9d6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(teacher.summary())\n",
    "\n",
    "teacher.fit(X_train, y_train, epochs = 100, batch_size = 64, validation_data = (X_test, y_test))\n",
    "teacher.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979dcb42-74f0-40ea-9019-c96ae9f0510e",
   "metadata": {},
   "source": [
    "Ahora pasamos a destilar el conocimiento de la red profeosra a la red estudiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940a9c8-c8b6-4444-8f04-d0faa3e1d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "# distiller.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     metrics=[keras.metrics.Accuracy()],\n",
    "#     student_loss_fn=keras.losses.BinaryCrossentropy(),\n",
    "#     distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "#     alpha=0.1,\n",
    "#     temperature=10,\n",
    "# )\n",
    "\n",
    "distiller.compile(optimizer = 'adam', metrics = ['accuracy'], student_loss_fn = keras.losses.BinaryCrossentropy(), distillation_loss_fn = keras.losses.BinaryCrossentropy(), alpha = 0.1, temperature = 10)\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(X_train, y_train, epochs=100, batch_size = 64, validation_data = (X_test, y_test))\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(X_test, y_test)\n",
    "\n",
    "# weights = distiller.get_weights()\n",
    "# print(weights)\n",
    "# print(student.summary())\n",
    "\n",
    "# model.save(\"distilled_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485ae2c-fa7b-4cab-9789-4e5f2756bb46",
   "metadata": {},
   "source": [
    "Ahora vamos a entrenar la red estudiante por separado para ver qué beneficios trae usar knowledge distillation en este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4959a-fef2-4d37-b1f3-d02ba615d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_scratch.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "student_scratch.fit(X_train, y_train, epochs=100, batch_size = 64, validation_data = (X_test, y_test))\n",
    "student_scratch.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0efeb-f823-456b-a1e6-71d61584e1bb",
   "metadata": {},
   "source": [
    "Luego de aplicar la técnica de compresión de Knowledge Distillation se evidencia que el conocimiento se transmite de forma satisfactoria de la red profesora a la red estudiante perdiendo únicamente un punto de precisión. Sin embargo, al entrenar la red estudiante directamente observamos que se obtiene una precisión mucho más cercana a la de la red profesora. Esto quiere decir que a pesar que la transferencia de conocimiento de una red a otra es posible, pero no es eficiente en este caso. Por lo tanto, descartaremos esta técnica de compresión debido a que no está aportando valor a este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae3c93-6b0c-4594-b718-679f30bf89c9",
   "metadata": {},
   "source": [
    "## Técnicas de Compresión - Cuantización\n",
    "\n",
    "Por último, vamos a revisar la cuantización de los modelos como técnica de compresión. Esta técnica consiste en cambiar la representación de los pesos, biases y/o entradas con el fin de simplificar las operaciones que se realizan en el proceso de inferencia o para disminuir el tamaño de los modelos al disminuir la cantidad de bits con los que se representan estos números. Normalmente, la cuantización viene acompañada de pérdidas en precisión y es precisamente lo que se va a evaluar en este ejemplo. Acá se hará la conversión a modelos de TFLite con cuantización a Float16 (fp16) y de rango dinámico (drq) y se va a evaluar su desempeño en el microcontrolador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2120efa-96b5-4130-8940-ac8f428b6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión del modelo - Cuantización con Float16\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"modelo_base\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "open(\"modelo_cuantizado_fp16.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# Conversión del modelo - Cuantización con Dynamic Range\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"modelo_base\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "open(\"modelo_cuantizado_drq.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
